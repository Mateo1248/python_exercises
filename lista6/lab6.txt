Lista 6 (Lab) Termin wysłania na SVN do 24.05.2020 31.05.2020
Na tej liście nie korzystamy z bibliotek uczenia maszynowego np. keras. Jej celem jest poznanie algorytmów uczenia sieci neuronowych.

zad1.
(10pt) Zaimplementuj algorytm propagacji wstecznej przedstawiony na wykładzie 9 dla sieci neuronowej 3-4-1 (3-wejścia, 4-warstwa ukryta, 1-wyjście). Rozwiąż pokazany problem XOR. Następnie
zamiast funkcji sigmoidalnej wykorzystaj funkcje aktywacji ReLU, wykorzystaj kombinacje funkcji sigmoidalnej, lub inne kombinacje. Poeksperymentuj!
Które rozwiązanie daje lepszą dokładność? Odpowiedź na to pytanie pisząc program testowy, który to pokazuje (np. po uruchomieniu w terminalu dostajemy wyniki dla różnych kombinacji z opisem). Rozwiąż ten problem również dla innych funkcji logicznych AND i OR. Dlaczego w danych wejściowych z przykładu z wykładu mamy ostatnią kolumnę z samymi jedynkami. Na to pytanie odpowiedź w komentarzu kodu źródłowego programu.
Dodatkowa uwaga: Część osób rozwiązując zadanie zauważyła (dostaje dużo pytań w tej sprawie), że możemy dostawać różne wyniki np. w implementacji z wykładu zmieniając 'sigmoid' na 'relu' i uruchamiając program za każdym razem możemy dostawać zbieżność do wektora np. [0.5, 0.5, 0.5, 0.5] lub [0, 0, 0, 0] lub to co oczekujemy czyli wartości bliskie funkcji XOR. Ten efekt to jest standardowy problem wykorzystywanych algorytmów uczenia (spadek gradientu) sieci neuronowych. Takie same wyniki otrzymamy w Kerasie (ostatni slajd) po zmianie z 'sigmoid' na 'relu' (nawet dla innych algorytmów uczenia). Po prostu wszystko zależy od wag początkowych, które w naszym przypadku (keras też tak robi) są losowe. Ale proszę zobaczyć np.

    def relu(x):
        return x * (x > 0)

    def relu_derivative(x):
        return 1. * (x > 0)

    np.random.seed(17)  # początkowy wybór wag WAŻNE

    class NeuralNetwork:
        ...
        self.eta = 0.01  # tak ważne jest zmniejszyć krok
        ...

    Przed print'ami dodajmy line z ustaleniem precyzji
        np.set_printoptions(precision=3, suppress=True)
        print(nn.output)
        print(nn.weights1)
        print(nn.weights2)
Wtedy "powinno" ładnie zbiegać do tego co oczekujemy. Po prostu wszystko zależy od początkowych wag i akurat dla np.random.seed(17) "udaje się". W innych przypadkach (inny seed) często wpada w lokalne minimum i nie wyskakuje (keras ma tak samo proszę to sprawdzić!). To nie jest tylko przypadek dla 'relu' dla przykładu z wykładu proszę tylko zmienić eta na 
0.001
 i już wpadniemy w [0.5, 0.5, 0.5, 0.5]. Podsumowując, wyniki warto robić nie tylko dla samych wyborów funkcji aktywacji, ale również wyboru wag i kroku eta! Ale w przypadku tego zadania i następnego wystarczy oddać tylko "dobre" przypadki, czyli dla odpowiednich wyborów np.random.seed i kroku eta przy ustalonych funkcjach aktywacji. Więcej informacji na ten temat.

zad2.
(15pt) Zaprojektuj sieć neuronową dwuwarstwową do aproksymacji funkcji np. sieć 1-5-1 lub 1-10-1 itp. i naucz ją funkcji (jedna funkcja dla jednej sieci)
Weźmy dane wejściowe, jako próbkowana funkcja paraboliczna
>>> x=np.linspace(-50,50,26)
>>> y=x**2
>>> plt.scatter(x,y)
Sieć testuj dla wektora wejściowego
>>> x=np.linspace(-50,50,101)
>>> y - wynik sieci dla wejścia x
>>> plt.scatter(x,y)
Weźmy dane wejściowe, jako próbkowana funkcja sinus
>>> x = np.linspace(0,2,21)
>>> y = np.sin((3*np.pi/2) * x)
>>> plt.scatter(x,y)
Sieć testuj dla wektora wejściowego
>>> x = np.linspace(0,2,161)
>>> y = wynik sieci dla wejścia x
>>> plt.scatter(x,y)
Dobierz też odpowiednie dla problemu funkcje aktywacji! Dokładnie sigmoidalna, relu lub tanh. W czasie uczenia pokazuj aproksymacje funkcji co np. 100 krok, czyli co 100 krok np. dla funkcji parabolicznej dla wektora x = np.linspace(-50,50,101) pokaż wynik działania sieci wykorzystując matplotlib. Dostaniemy animacje procesu uczenia sieci. Wyświetlaj jeszcze aktualny krok uczenia oraz błąd średnio-kwadratowy. Animacje procesu uczenia wykonaj dla dwóch powyższych zbiorów danych (parabola i sinus).

Przykładowa animacja procesu uczenia dla paraboli (jaką powinien wykonywać program):

zad3.
(20pt)* Wykonaj zadanie drugie dla sieci o trzech warstwach np. 1-10-10-1. Sam napisz algorytm propagacji wstecznej dla sieci trójwarstwowej. Jakie wyniki aproksymacji dostaniemy dla tej ,,głębszej'' sieci?
